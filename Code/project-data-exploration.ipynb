{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: <span style=\"color:blue\"> *Azaan Patil* </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import warnings\n",
    "from pdb import set_trace\n",
    "\n",
    "# Default seed\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13ec3e86aaf10ea4371b8b6c2fbadfb7",
     "grade": true,
     "grade_id": "cell-a9c5485765365762",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TodoCheckFailed(Exception):\n",
    "    pass\n",
    "\n",
    "def todo_check(asserts, mute=False, success_msg=\"\", **kwargs):\n",
    "    locals().update(kwargs)\n",
    "    failed_err = \"You passed {}/{} and FAILED the following code checks:\\n{}\"\n",
    "    failed = \"\"\n",
    "    n_failed = 0\n",
    "    for check, (condi, err) in enumerate(asserts):\n",
    "        exc_failed = False\n",
    "        if isinstance(condi, str):\n",
    "            try:\n",
    "                passed = eval(condi)\n",
    "            except Exception:\n",
    "                exc_failed = True\n",
    "                n_failed += 1\n",
    "                failed += f\"\\nCheck [{check+1}]: Failed to execute check [{check+1}] due to the following error...\\n{traceback.format_exc()}\"\n",
    "        elif isinstance(condi, bool):\n",
    "            passed = condi\n",
    "        else:\n",
    "            raise ValueError(\"asserts must be a list of strings or bools\")\n",
    "\n",
    "        if not exc_failed and not passed:\n",
    "            n_failed += 1\n",
    "            failed += f\"\\nCheck [{check+1}]: Failed\\n\\tTip: {err}\\n\"\n",
    "\n",
    "    if len(failed) != 0:\n",
    "        passed = len(asserts) - n_failed\n",
    "        err = failed_err.format(passed, len(asserts), failed)\n",
    "        raise TodoCheckFailed(err.format(failed))\n",
    "    if not mute: print(f\"Your code PASSED all the code checks! {success_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instructions\n",
    "In this assignment, you will begin to experience and practice some of the initial stages of the ML pipeline: problem formulation, data gathering, and data visualization/exploration.\n",
    "\n",
    "Your job is to read through the assignment and fill in any code segments that are marked by `TODO` headers and comments. Some TODOs will have a `todo_check()` function which will give you a rough estimate of whether your code is functioning as excepted. Other's might not have these checks, like visualization TODOs. Regardless,  all the correct outputs are given below each code cell. It might be useful to copy the contents of certain TODO cells into a new cell so you can try to match the desired output with the output produced by your own code! For visualization TODOs, you simply have to have a plot that looks similar. You can change aspects such as color, titles, or x/y-axis labels if you so wish.\n",
    "\n",
    "At any point, if you feel lost concerning how to program a specific TODO, take some time and visit the official documentation for each library and read about the methods/functions that you need to use.\n",
    "\n",
    "## Submission\n",
    "\n",
    "1. Save the notebook.\n",
    "2. Enter your name in the appropriate markdown cell provided at the top of the notebook.\n",
    "3. Select `Kernel` -> `Restart Kernel and Run All Cells`. This will restart the kernel and run all cells. Make sure everything runs without errors and double-check the outputs are as you desire!\n",
    "4. Submit the `.ipynb` notebook on Canvas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "\n",
    "<center><img src=\"https://insideclimatenews.org/wp-content/uploads/2023/03/wildfire_thibaud-mortiz-afp-getty-scaled.jpg\" alt=\"drawing\" width=\"500\"/></center>\n",
    "\n",
    "For this assignment, you will be tasked with exploring data related to the prediction of forest fires, a major environmental concern that affects forest preservations around the world.\n",
    "\n",
    "However before getting into the data itself, it is important that we demonstrate how to properly formulate the goal of this assignment. Recall, that when formulating a problem, it is useful to ask yourself the following three questions:  \n",
    "\n",
    "1. What is the problem?\n",
    "2. Why does the problem need to be solved?\n",
    "3. How would you solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the problem?\n",
    "Well, as you might have already read, this assignment is aiming to tackle is forest fire prediction. To be more specific, let's focus on predicting forest fires in particular parks, like national parks. By doing so, we will simplify the problem by focusing on a specific park. The big assumption here is going to be that it will be possible to predict forest fires from data. But what kind of data do we need? If we do a little bit of research by reading research papers and talking with experts on the subject, we'll find that natural forest fires actually tend to coincide with the weather (what a shock)!\n",
    "\n",
    "For instance, natural fires tend to coincide with high temperatures, dry conditions, and wind. All of which tend to be commonly recorded features in meteorological datasets [\\[1\\]](https://mylandplan.org/content/facts-about-fire)! Thus, the goal will be to find a meteorological dataset for a specific national park."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does the problem need to be solved?\n",
    "Why are we even looking at this problem to begin with. Well, to some of you this might be obvious, but let's make it clear anyway. \n",
    "\n",
    "One point of forest fire prediction is to help prevent fires by either predicting likely areas where they may occur or predicting how much a potential area could burn (identifying high risk areas). By making these predictions, we hope to allow for faster detection of fires by focusing on particular \"high risk\" areas and, in turn, becoming more successful at forest fire prevention. \n",
    "\n",
    "Now, this is just one way to frame why this problem needs to be solved. Feel free to think about your own personal take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How would you solve the problem?\n",
    "To answer this question, think about how you would go about achieving or solving the problem (try to visualize what you would do for each of those 7 ML steps). Since you are likely new to ML, we will try to walk you through our thought process.\n",
    "\n",
    "The first step you might think about is how would you gather data? Well, we tried to answer this question earlier. Recall, we talked about narrowing our focus down to national parks. The next step would be to contact parks and see if they do indeed have any open source data (particularly meteorological data). Further, we could search for related problems to see if they already have datasets, or we could simply try searching for a dataset related to forest fires. We will also have to make sure the dataset is labeled so that we can train our model to predict coordinates for which parts of the forest are likely to be vulnerable to fires or how much of an area will be burned.\n",
    "\n",
    "Say we found some data, then what? What features will our data even have? We mentioned we would like some sort of meteorological data that maybe contains information about temperature, moisture, wind, etc. But are there other domain specific features out there? As none of us are forest fire expert, we probably do not have any clue if there are! Take note as this is a place of uncertainty that we have identified and would need to further investigate.\n",
    "\n",
    "Okay, so let's also say we found some data with useful meteorological and domain specific features (whatever those may be). Now, what assumptions can we make about our data? Do we know what algorithm might actually be well suited for our problem? Notice, that these questions are becoming hard and harder to answer as we attempt to plan further into the future. Once answers start becoming this vague, it might be a good idea to get your hands dirty by finding some data and exploring it using these questions as guides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In summary, our goal is to help prevent and make firefighting easier by identifying \"high risk\" areas by either predicting likely areas where forest fires may occur or predicting how much a potential area could burn when a forest fire occurs. We think we can achieve these predictions by using some sort of labeled data where the labels are either coordinates for which parts of the forest have burned before or how much of an area was burned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering and Loading\n",
    "In this assignment, you will be working with the House Listings Dataset. This is a real-world dataset containing information about residential properties listed for sale. The dataset includes various features such as property characteristics (number of bedrooms, bathrooms, square footage), location information, and the sale price.\n",
    "\n",
    "This dataset provides an excellent opportunity to explore the relationships between property features and their market values, which is a fundamental problem in real estate analysis and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "Before loading the data, you will need to import any Python libraries you think will be needed. For data handling (loading, storing, and manipulation), you can use either NumPy or Pandas. For plotting, you can use Matplotlib and Pandas (which has wrappers for Matplotlib which makes plotting super easy). \n",
    "\n",
    "Recall that Pandas is a high-level data manipulation and analysis tool built on-top of NumPy. In particular, it can be easier to work with when cleaning, visualizing, and preprocessing data. Plus, Pandas tends to be easier on the eyes when visualizing raw datasets. \n",
    "\n",
    "- [Pandas docs](https://pandas.pydata.org/pandas-docs/stable/reference/index.html).\n",
    "- [NumPy docs](https://numpy.org/doc/stable/)\n",
    "- [Matplotlib docs](https://matplotlib.org/stable/contents.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'conda' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mconda install -y numpy pandas matplotlib\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading\n",
    "The House Listings dataset file `House_listings_dataset.csv` should be located in the **SAME** directory/folder as this notebook. If you do not have it, please ensure it is placed in the correct location before proceeding.\n",
    "\n",
    "To display the local path of this notebook and the directory it is currently in, run the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The current path for your notebook is:\\n {os.getcwd()}\\n\")\n",
    "print(f\"Your notebook is currently in the following directory:\\n {os.path.basename(os.getcwd())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 1 (5 points): Data Loading and Displaying\n",
    "Load the `House_listings_dataset.csv` by using Pandas `read_csv()` function. The  `read_csv()` function works by taking in a path to a csv file (e.g., `/home/user/Downloads/House_listings_dataset.csv`). However, since you should have moved the data into the same directory as this notebook, you will only need to pass the name of the csv `House_listings_dataset.csv`. \n",
    "\n",
    "**WARNING: Do NOT pass a path specific to your computer! If you do, the TA's will not be able to run your notebook without making changes to it. This is why we are having you move the dataset into the same directory as this notebook.**\n",
    "\n",
    "1. Load House Listings dataset by passing the name of the csv file \"House_listings_dataset.csv\" to the Pandas function `read_csv()` ([docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)). Store the output into the `df` variable.\n",
    "\n",
    "2. Using the `df` DataFrame you just defined, call the `columns` class variable to get all the column/feature names. Store the output into the variable `feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a56409d0fb3fbee8c4dc2923893a78d",
     "grade": false,
     "grade_id": "cell-17a0d8a6a2cdbdb5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 1.1\n",
    "df = pd.read_csv(\"House_listings_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02a16c24f44fe4303b848d2da4829056",
     "grade": true,
     "grade_id": "cell-730d95f9cb9bc620",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"os.path.exists('House_listings_dataset.csv')\", f\"The House_listings_dataset.csv is not detected in your local path! You need to move the 'House_listings_dataset.csv' file to the same location/directory as this notebook which is {os.getcwd()}\"),\n",
    "    (\"len(df) > 0\", 'The dataset was not loaded correctly!')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the `forestfire_df` you can simply print it using Python's `print()` function. However, this does not look very nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, you can pretty print a Pandas DataFrame using Jupyter Lab/Notebooks built-in function `display()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6efa135ce912b82a4545b4581c56c583",
     "grade": false,
     "grade_id": "cell-965da1c9bbc182a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 1.2\n",
    "feature_names = df.columns\n",
    "\n",
    "print(f'The feature names are:\\n{feature_names.values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97037fe83efda88799f75c8e30f74e31",
     "grade": true,
     "grade_id": "cell-f05e515667a866db",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"len(feature_names) > 0\", \"No feature names detected!\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and Exploration\n",
    "\n",
    "## Defining the features\n",
    "\n",
    "Once you have loaded the data, it is time to understand what the provided features even mean.If you paid close attention, the web-page does provide a brief description of each feature. For your convince, the descriptions are posted below.\n",
    "\n",
    "    X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "    Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "    month - month of the year: 'jan' to 'dec'\n",
    "    day - day of the week: 'mon' to 'sun'\n",
    "    FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "    DMC - DMC index from the FWI system: 1.1 to 291.3\n",
    "    DC - DC index from the FWI system: 7.9 to 860.6\n",
    "    ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "    temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "    RH - relative humidity in %: 15.0 to 100\n",
    "    wind - wind speed in km/h: 0.40 to 9.40\n",
    "    rain - outside rain in mm/m2 : 0.0 to 6.4\n",
    "    area - the burned area of the forest (in ha): 0.00 to 1090.84\n",
    "    \n",
    "Some of the features are pretty straight forward to understand like temp, RH (relative humidity), wind, rain, area, month, and day. But what about these other features like X, Y, FFMC, DMC, DC, and ISI which are more technical? \n",
    "\n",
    "X and Y are x-y coordinates that correspond with the below image that shows off Montesinho natural park.\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Paulo-Cortez-4/publication/238767143/figure/fig1/AS:298804772392991@1448252017812/The-map-of-the-Montesinho-natural-park.png\" width=500 height=500></center>\n",
    "\n",
    "For the other variables (FFMC, DMC, DC, and ISI), you actually have to take a look at the [research paper](http://www.dsi.uminho.pt/~pcortez/fires.pdf) to get a better explanation. To save you some time, here is what it says.\n",
    "\n",
    "> The forest Fire Weather Index (FWI) is the Canadian system for rating fire danger\n",
    "and it includes six components (Figure 1) [24]: Fine Fuel Moisture Code (FFMC),\n",
    "Duff Moisture Code (DMC), Drought Code (DC), Initial Spread Index (ISI), Buildup\n",
    "Index (BUI) and FWI. The first three are related to fuel codes: the FFMC denotes the\n",
    "moisture content surface litter and influences ignition and fire spread, while the DMC\n",
    "and DC represent the moisture content of shallow and deep organic layers, which affect\n",
    "fire intensity. The ISI is a score that correlates with fire velocity spread, while BUI\n",
    "represents the amount of available fuel. The FWI index is an indicator of fire intensity\n",
    "and it combines the two previous components. Although different scales are used for\n",
    "each of the FWI elements, high values suggest more severe burning conditions. Also,\n",
    "the fuel moisture codes require a memory (time lag) of past weather conditions: 16\n",
    "hours for FFMC, 12 days for DMC and 52 days for DC.\n",
    "\n",
    "In summary, it sounds like these remaining variables are domain specific variables related to a system for rating fire danger based on things like intensity, spread, and potential fuel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring\n",
    "\n",
    "Given your rough understanding of the features, it is time to start to developing a better intuition for each feature by exploring their values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 2 (5 points): Data Info \n",
    "When exploring a new dataset, you should first figure out how many data samples and features are present and the data type of each feature.\n",
    "\n",
    "1. Get the shape of the `forestfire_df` DataFrame (which contains the number of data samples and features) by calling the `shape` class variable. Store the output into `ff_shape`.\n",
    "   \n",
    "2. Display a short the summary of the data by calling the `info()` method for the  `forestfire_df` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f4c73b111d00cbb3e90e43d53338529",
     "grade": false,
     "grade_id": "cell-51b34baab6af54b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 2.1\n",
    "ff_shape = df.shape\n",
    "\n",
    "print(f'The house listings dataset shape is: {ff_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "139c6b299e1f0326135ad378fadaadc2",
     "grade": true,
     "grade_id": "cell-79f451a6be7e8b50",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"ff_shape[0] > 0 and ff_shape[1] > 0\", 'The shape does not appear to be correct')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the dataset indicates the number of data samples (rows) and features (columns) available in the house listings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cce8dfd78ccb43410dad874a3ab584dd",
     "grade": true,
     "grade_id": "cell-2de1c31685ffffc0",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 2.2\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output of the `info()` method, you should be able to see that all the features are either integers or floats, except for the 'month' and 'day' features. You can tell this by looking at the \"Dtype\" column, which stands for data types ([dtype docs](https://numpy.org/doc/stable/reference/arrays.dtypes.html)). It seems the 'month' and 'day'  are classified as an \"object\". When Pandas or NumPy encounters a string, it will assign it the dtype \"object\". Thus, 'month' and 'day' are strings (this can be confirmed by looking at the values of month/day in the displayed `forestfire_df` provided above).\n",
    "\n",
    "Lastly, it is worth noting that `info()` is reporting that there are no null values in any of our features columns, as the \"Non-Null Count\" is empty for each feature! That's good, that means there are no missing values, and you will explore this manually later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 3 (10 points): Month and Day Visualization\n",
    "\n",
    "Take a closer look at 'month' and 'day' features to see if you can gain any further insights about the data. For these TODOs use either `iloc`, `loc` or square brackets `[ ]` to slice/index the `forestfire_df` DataFrame.\n",
    "\n",
    "1. Index the 'month' column from our `forestfire_df` feature and call the `value_counts()` method ([docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html)) on said column data to get the number of times each month appears in the data. Store the output into the variable `month_counts`.\n",
    "\n",
    "2. Plot `month_counts` using the Matplotlib bar plot function `plt.bar()` ([docs](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html)) so that the months are displayed on the x-axis and the month counts are displayed on the y-axis.\n",
    "    1. Hint: To easily accesses the month names, use the `.index` class variable for `mounth_counts`.\n",
    "\n",
    "3. Index the 'day' column from our `forestfire_df` feature and call the `value_counts()` method on said column data to get the number of times each day appears in the data. Store the output into the variable `day_counts`.\n",
    "\n",
    "4. Plot `day_counts` using the Matplotlib bar plot function `plt.bar()` ([docs](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html)) so that the days are displayed on the x-axis and the day counts are displayed on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "153343043de690fb968502da746a1cc5",
     "grade": false,
     "grade_id": "cell-5a60295f367e2f5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 3.1\n",
    "state_counts = df[\"State\"].value_counts()\n",
    "\n",
    "display(state_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f79dbef8daaec9b23b8bf8c2ab1fda5",
     "grade": true,
     "grade_id": "cell-5353f939c4dc9647",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"state_counts.shape[0] > 0\", 'State values did not load correctly!')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c620ad331781caeb273404f86545e6a",
     "grade": true,
     "grade_id": "cell-f55be628a39f74fd",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 3.2\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(state_counts.index, state_counts.values)\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"House Listings by State\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of house listings across states gives us insight into the geographic concentration of the data. Some states may have significantly more listings than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3701ca18fbe9a8f4fc39e4ef467a0da3",
     "grade": false,
     "grade_id": "cell-b798fc15675fe3b2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 3.3\n",
    "bedroom_counts = df[\"Bedroom\"].value_counts().sort_index()\n",
    "\n",
    "display(bedroom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4c6b90fab27f8075cfeec818b272683",
     "grade": true,
     "grade_id": "cell-ec0ba5d7018f8288",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"bedroom_counts.shape[0] > 0\", 'Bedroom values did not load correctly!')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffad8b119793c1b269fe60ad159faf3e",
     "grade": true,
     "grade_id": "cell-29d1cd2b81aeaf2b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 3.4\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(bedroom_counts.index, bedroom_counts.values)\n",
    "plt.xlabel(\"Number of Bedrooms\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Bedrooms in House Listings\")\n",
    "plt.xticks(bedroom_counts.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of bedrooms shows the variety of house types available in the dataset. Most houses tend to have a typical number of bedrooms, with varying frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 4 (10 points): Null Check\n",
    "\n",
    "While the `info` method used previously indicated that there were no nulls or missing values, it is possible to check manually. If there are any nulls or missing values, then these will need to be dealt with during the data cleaning phase by filling them or dropping the entire data sample.\n",
    "\n",
    "1. Convert our `forestfire_df` into a boolean DataFrame (true indicates a value is a missing) using the Pandas DataFrame `isnull()` method ([docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html)). Store the output into the `forestfire_isnull` variable.\n",
    "\n",
    "It's hard to see if every single element in the dataset is false. If only there was an easier way. Well, there is! You can use the NumPy function `any()` ([docs](https://numpy.org/doc/stable/reference/generated/numpy.any.html)) to check if every boolean in `forestfire_isnull` is false. In other words, `any()` will only return true if at least one element in `forestfire_isnull` is true.\n",
    "\n",
    "2. Use the NumPy `any()` function to determine if `forestfire_isnull` has any True values indicating there are null elements in the data. Store the output into the variable `hasnull`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc423b69c8dc744233e9e71a439efd79",
     "grade": false,
     "grade_id": "cell-760f2e7a973a24e7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 4.1\n",
    "df_isnull = df.isnull()\n",
    "\n",
    "display(df_isnull.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3beaa57f1c003f4a3c64f4a7328049c0",
     "grade": true,
     "grade_id": "cell-459e5ebe3f1e5c0e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"df_isnull.shape[0] > 0\", \"df_isnull shape does not match expected\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e39b8321adbb1f2aa3d0dbce51946d8e",
     "grade": false,
     "grade_id": "cell-f42b36305116404a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 4.2\n",
    "hasnull = np.any(df_isnull)\n",
    "\n",
    "print(f\"Dataset contains null values: {hasnull}\")\n",
    "\n",
    "# Count null values per column\n",
    "print(\"\\nNull values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1f56473d041f5ed50ae79eb4524cb45",
     "grade": true,
     "grade_id": "cell-ae287233b9eff506",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"isinstance(hasnull, (bool, np.bool_))\", 'hasnull should be a boolean value!')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The house listings dataset contains some missing data. Several columns have null values that will need to be addressed during preprocessing through imputation or removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### TODO 5 (15 points): Data Statistics\n",
    "\n",
    "Next up, we need to check out the statistics of the numerical features in the data, such as the mean and standard deviation. Such scale information will prove to be vital in the future, as ML algorithm can have a hard time learning if the scales of all the features are different. For instance, an algorithm might end up valuing a feature more just because it has a larger scale, which is not what we want.\n",
    "\n",
    "\n",
    "1. Use the Pandas DataFrame `describe()` method on our `forestfire_df` to get a statistical summary for each of our numerical features. Store the output into the variable `ff_describe`.\n",
    "\n",
    "2. Using a box plot, visualize the statistics of the data by calling the Pandas `.boxplot()` method ([docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html)) for `forestfire_df`.\n",
    "\n",
    "3. Visualize each numerical feature's box plot separately by creating a for-loop which loops over all column names (i.e., features) in `forestfire_df` and plots a single box plot for the current column by passing the column name to the `boxplot()` method (you used this method in the previous step).\n",
    "    1. Hint: You will need to avoid plotting the 'day' and month 'columns' as they are non-numerical. \n",
    "    2. Hint: Be sure to add `plt.show()` after plotting within the for-loop, or all the plots will be plotted on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c09201fd8ad0ec4407b0d496856eb3b",
     "grade": false,
     "grade_id": "cell-afb567980b7ed9a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 5.1\n",
    "ff_describe = df.describe()\n",
    "\n",
    "display(ff_describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "301907f84c17e79d1d387ce96697d1d2",
     "grade": true,
     "grade_id": "cell-3184fc963539f217",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"ff_describe.shape[0] > 0\", 'ff_describe did not compute correctly'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that nearly every feature has a different mean and STD. For example, the 'DC' features has a mean of ~547 while the rain feature has a mean of ~ 0.0216.  This is an indication that the features have different scales!\n",
    "\n",
    "Further, take note of how large the standard deviation or [STD](https://en.wikipedia.org/wiki/Standard_deviation) is for some of the features. If you have a large mean and comparably large STD, this is not necessarily a problem. However, if you have a small mean and a very large STD, this can cause trouble for learning as the values for said feature will vary drastically. For instance, the area has a small mean and a much larger STD. Keep this in mind as you'll investigate this more soon.\n",
    "\n",
    "As a side note, the `describe()` method also provides the min, max, count, and the [percentiles](https://www.w3schools.com/python/python_ml_percentile.asp) values.\n",
    "\n",
    "Finally, notice that the 'day' and 'month' features are not included. This is because their values are non-numerical (in this case they are string values) and computing the statistics is impossible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a6af17005ee7cc7a6ff9953f257daa0",
     "grade": true,
     "grade_id": "cell-e2272839d1eb2cc6",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 5.2\n",
    "df.boxplot(figsize=(15, 6))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For those that do not know how to read a box plot, refer to this blog [*Box Plot Explained*](https://www.simplypsychology.org/boxplots.html). Any black circle here indicates an outlier or anything above the maximum or below the minimum (i.e., the whiskers). \n",
    " \n",
    " As you should see, this is not a very informative plot because all the features have different scales. So, features with small scales are all squashed towards 0, making them unreadable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b52148377ef82fa074e11fc516a49a9f",
     "grade": true,
     "grade_id": "cell-d7ea5a8cbcba60c5",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO 5.3\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    df.boxplot(column=col, figsize=(10, 5))\n",
    "    plt.title(f\"Box Plot of {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 6 (10 points): Price Visualization\n",
    "\n",
    "Now it's time to take a closer look at the target/label, the 'Price' column. To do so, you need to examine the actual values and understand the price distribution.\n",
    "\n",
    "1. Store the 'Price' data into a separate variable. Do so by indexing the 'Price' column using Pandas' `iloc`, `loc` or square brackets `[ ]`. Store the output into the variable `price_values`.\n",
    "   \n",
    "2. To more easily visualize the 'Price' values, create a histogram. First remove NaN values, then plot `price_values` using the Matplotlib `plt.hist()` function ([docs](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html)).\n",
    "    1. Hint: The only required parameters are `x` for the function. The `bins` parameter is optional and will automatically be computed using the passed data. \n",
    "\n",
    "3. Call the `describe()` method for `price_values` to get summary statistics of house prices. Store the output into the variable `price_stats`.\n",
    "\n",
    "4. Examine the price distribution and note any patterns or outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8a2a78b4f8dd722e019a7dacf0a2695",
     "grade": false,
     "grade_id": "cell-1101f14aa6fa25df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 6.1\n",
    "price_values = df[\"Price\"]\n",
    "\n",
    "display(price_values.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59eb4942ca34de5311d545fe5a4c2e5b",
     "grade": true,
     "grade_id": "cell-de12a3df5573e8c2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"len(price_values) > 0\", 'price_values did not load correctly!'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from the quick printout of the values for `area_values` you should already be noticing lots of zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23862c9cefbc0645c700de8751809119",
     "grade": true,
     "grade_id": "cell-b9e2a6b26540405a",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 6.2\n",
    "# Remove NaN values for visualization\n",
    "price_clean = price_values.dropna()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(price_clean, bins=50)\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of House Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the majority of the area values are centered around 0 such that the data seems to be [skewed positively or to the right](https://www.investopedia.com/terms/s/skewness.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60385af1d082e4d5fe230ade60e0d247",
     "grade": false,
     "grade_id": "cell-1d7067aa131ebed9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 6.3\n",
    "price_stats = price_values.dropna().describe()\n",
    "\n",
    "display(price_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc6fbc6621f3b1f4197f44ccf2087b3d",
     "grade": true,
     "grade_id": "cell-58a1645c7767780f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"len(price_stats) > 0\", 'price_stats did not compute correctly!')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f459c36996a4bc7fd2acfb92f667efa0",
     "grade": false,
     "grade_id": "cell-6acfa11ea747e712",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 6.4 - observe statistics\n",
    "print(f\"Price Statistics:\")\n",
    "print(f\"Count: {price_stats['count']:.0f}\")\n",
    "print(f\"Mean: ${price_stats['mean']:,.2f}\")\n",
    "\n",
    "print(f\"Std: ${price_stats['std']:,.2f}\")print(f\"Max: ${price_stats['max']:,.2f}\")\n",
    "print(f\"Min: ${price_stats['min']:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "207f89cabf0396a0b187b86372f45201",
     "grade": true,
     "grade_id": "cell-86bb0c7f4d687cd7",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"price_stats.shape[0] > 0\", 'price_stats did not compute correctly!')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking at the exact numbers, you should be able to see that a majority (247 samples) of the area values are zero (no forest area was burned). However, further take note that the maximum value is 1090.84 and that there are many values reported in between 0 and 1090, yet they all have a count near 1. This spread of the data, with the majority of the data samples having an area of 0, is likely the cause of the data having a small mean but a large STD. As such, the 'area' data is positively or right skewed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 7 (10 points): Price Transformation\n",
    "\n",
    "Data transformation can help normalize distributions and improve model performance. The [logarithmic transformation](https://machinelearningmastery.com/skewness-be-gone-transformative-tricks-for-data-scientists/) is a common technique for handling skewed data by taking the $\\log$ of values to spread out the distribution and make it more [Gaussian-like](https://en.wikipedia.org/wiki/Normal_distribution).\n",
    "\n",
    "1. Apply the log transform to `price_clean` (cleaned price values without NaNs) by passing it to NumPy's `log()` function. Add 1 before taking the log to handle any edge cases. Store the output into the variable `log_price_values`.\n",
    "\n",
    "2. Create a histogram using the log-transformed values stored in `log_price_values` using the Matplotlib `plt.hist()` function ([docs](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html)).\n",
    "\n",
    "3. Call the `describe()` method for `log_price_values` to get a statistical summary of the log-transformed price values. Store the output into the variable `log_price_describe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58f5094ab09753c945d820e8a81ac6c5",
     "grade": false,
     "grade_id": "cell-14741c874eea594c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 7.1\n",
    "log_price_values = np.log(price_clean + 1)\n",
    "\n",
    "display(log_price_values.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f450630884545b9a9108ec2a974b273c",
     "grade": true,
     "grade_id": "cell-e7354c18d2e68e7c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"len(log_price_values) > 0\", 'log_price_values was not computed correctly.'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e62969fe4c6583cfc52f6a7a859d626",
     "grade": true,
     "grade_id": "cell-658304556e349ca1",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO 7.2\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(log_price_values, bins=50)\n",
    "plt.xlabel(\"Log Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Log-Transformed House Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should be able to see that the value range (the x-axis) for the 'area' values has been greatly condensed. This should allow to more easily see the spread of the counts for each value bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ba00ba9c7d45713496d3f6c74372cfb",
     "grade": false,
     "grade_id": "cell-e574aff1a6756b06",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 7.3\n",
    "log_price_describe = log_price_values.describe()\n",
    "\n",
    "display(log_price_describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8bbe864779e954c5ba9cadd67357e8c",
     "grade": true,
     "grade_id": "cell-7b6711efb0827e37",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"len(log_price_describe) > 0\", 'log_price_describe was not computed correctly!')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you take a look at the mean and STD you should see that the STD value more in line with the mean. Additionally, you should see that the max value has shrunk significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 8 (5 points): Feature Scatter Matrix\n",
    "Next, it is time to investigate the relationships between the features in the data and the distributions of each feature. It is important to investigate relationships to see how each feature compares with every other feature. In doing so, you can begin to observer trends in the data. Distributions are important to investigate in order to see how each feature is distributed - just like you did for target/label 'area'. \n",
    "\n",
    "To achieve both the comparison of features against one another and to observe the distributions of each feature, a handy tool called a scatter matrix can be employed. \n",
    "\n",
    "**WARNING: Plotting the scatter matrix can take a while!**\n",
    "\n",
    "1. Use Panda's `scatter_matrix()` function ([docs](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html)) to plot a scatter matrix. Pass `forestfire_df` as input. If the graph is too small, pass a larger `figsize` as an additional argument (e.g., `figsize=(15, 15)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2107ba36e6282ee06a3279516d778639",
     "grade": true,
     "grade_id": "cell-d026e3df8d921c2d",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 8.1\n",
    "# Select numeric columns only for scatter matrix\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Create a subset with key features for visualization\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(numeric_df.iloc[:, :6], figsize=(15, 15), alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read the docs all `scatter_matrix` does is plot each feature against one another. If a feature is plotted against its self, then the distribution over all the feature's values is given. For instance, the bottom right corner is the histogram you plotted for 'area'. Also notice that the plot is symmetric and 'day' and 'month' are not included. \n",
    "\n",
    "Now it can be hard to see any patterns as there is a lot of information being thrown at you. You might say there are some linear-like trends for the feature pairs  'DMC' and 'DC'. However, what might be more useful is looking at a reduced version of this graph, where you compare all the input features (everything but the target) against your target/label ('area' in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 9 (20 points): Area-Feature Scatter Matrix\n",
    "\n",
    "First, you must drop the 'area' column from the data so that only the input features remain. The `log_area_values` will be used in-place of the original 'area' columns values.\n",
    "\n",
    "1. Drop the 'area' column from the `forestfire_df` by calling the `drop()` ([docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html)) method and passing the name column name 'area'. Store the output into the variable `X`.\n",
    "    1. Hint: Be sure to pass the correct `axis` argument. `axis=0` corresponds to the rows or indexes, and `axis=1` corresponds to the columns. To know which one to use, think about whether 'area' is a row or column name.\n",
    "\n",
    "Next, you will need to create a subplot. A subplot is a plot that contains many smaller plots. As there are 12 features (not including 'area') the subplot must have 12 plots. One way this can be achieved is by having 3 rows and 4 columns of plots. Each `ax` will represent one plot, therefore, once `ax` is flattened, it should have a length of 12.\n",
    "\n",
    "2. Create a subplot by using Matplotlib's `plt.subplots()` function ([docs](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html)). The subplot should have 3 rows and 4 columns. Store the output into the variables `fig` and `ax`.\n",
    "    1. Hint: Look at the docs to see which argument is used to set the number of rows and columns.\n",
    "    2. Hint: If the graph is too small, pass a larger `figsize` as an additional argument (e.g., `figsize=(15,15)`).\n",
    "   \n",
    "**Inside the for-loop**\n",
    "\n",
    "The loop code is looping over the column names, where the current column name is stored in `column_name`. `enumerate` is a counter and stores the corresponding index of the current column name inside `idx`. Since there are 12 column names and 12 plots, `idx` can be used to index `ax`. \n",
    "\n",
    "3. Index the current `ax` using the variable `idx`. Store the output into `current_ax`. `current_ax` will represent the current plot you are plotting to.\n",
    "\n",
    "4. Create a scatter plot by calling the `plot()` method for `current_ax`. Pass the `log_area_values` as the x-values, the current feature data as the y-values, and the format string `'.'` (to make the plot a scatter plot).\n",
    "\n",
    "    1. Hint: You can get the current feature data by indexing `X` by the current column name `column_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f265ebd8ac7aad9d2386216894e7e453",
     "grade": true,
     "grade_id": "cell-93a54bfe1cc37359",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 9.1\n",
    "X = df.drop(\"Price\", axis=1)\n",
    "\n",
    "# Select only numeric features\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "print(f\"Features for plotting:\\n{list(numeric_features)}\")\n",
    "\n",
    "# TODO 9.2\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(15, 12))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Filter out NaN values in price\n",
    "price_clean_full = df[\"Price\"].dropna()\n",
    "df_clean = df.dropna(subset=['Price'])\n",
    "\n",
    "# TODO 9.3\n",
    "for idx, column_name in enumerate(numeric_features[:9]):\n",
    "    current_ax = ax[idx]\n",
    "    \n",
    "    # Filter out NaN values in the current feature\n",
    "    feature_clean = df_clean[column_name].dropna()\n",
    "    price_subset = price_clean_full[:len(feature_clean)]\n",
    "    \n",
    "    current_ax.plot(feature_clean, price_subset, '.')\n",
    "    current_ax.set_ylabel('Price')\n",
    "    current_ax.set_xlabel(column_name)\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the below picture of what different correlations can look like, do you see any? (the numbers above indicate the correlation coefficient which you will compute next).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1280px-Correlation_examples2.svg.png)\n",
    "\n",
    "Well, using this qualitative visualization, it does not look like the features have any obvious correlations with the 'area' targets. However, do note that it is hard to observe any correlations as many of the variables take on discrete values like the features 'X', 'Y', 'month' and 'day'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 10 (10 points): Correlation Matrix\n",
    "\n",
    "To quantitatively examine correlations between features and the target, compute a correlation matrix. Recall that the correlation coefficient ranges from -1 to 1: close to 1 means strong positive correlation, close to -1 means strong negative correlation, and near zero means very weak or no correlation.\n",
    "\n",
    "1. Call the `corr()` method ([docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)) for the `df` DataFrame to compute the correlation matrix of all **numerical** feature pairs. Store the output into the variable `corr_matrix`.\n",
    "    1. Pass the `numeric_only` argument to include only numerical features.\n",
    "<br><br>\n",
    "2. Index the 'Price' column of `corr_matrix` and store the output into the variable `price_corr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06f82b6f690cde393169a44b72b45059",
     "grade": false,
     "grade_id": "cell-0eded9015b8a72ea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 10.1\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "display(corr_matrix.style.background_gradient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cec517269e0a7f651bafe4842176a1b",
     "grade": true,
     "grade_id": "cell-108fef26204f32ab",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"corr_matrix.values.diagonal().sum() == 11.0\", 'corr_matrix values did not match!'),\n",
    "    (\"np.isclose(corr_matrix.iloc[0].sum(), 1.5719, rtol=0.01)\", 'corr_matrix values did not match!'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `.style.background_gradient()` you can color code the cells of `corr_matrix` where blue values correspond to positive correlations, red values correspond to negative correlations, and gray values correspond to no correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d849485696990125267dc7c8b0277a9a",
     "grade": false,
     "grade_id": "cell-c8df58b26562c498",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 10.2\n",
    "price_corr = corr_matrix[\"Price\"]\n",
    "\n",
    "display(price_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8cca44f043bd3c933572a1119ac7d3e",
     "grade": true,
     "grade_id": "cell-7230d2d12f623af0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "todo_check([\n",
    "    (\"np.isclose(price_corr['Price'], 1.0, rtol=0.01)\", 'price_corr values did not match!')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the correlation coefficients to understand which property features have the strongest relationships with house prices. Features with stronger correlations (further from 0) are likely to be better predictors of price."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "199.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
